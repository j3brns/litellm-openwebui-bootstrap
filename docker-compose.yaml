services:
    openwebui:
        # Using a specific tag for Open WebUI for better stability. Check for newer versions on https://github.com/open-webui/open-webui/releases
        image: ghcr.io/open-webui/open-webui:v0.6.10
        container_name: open-webui
        ports:
            - "3000:8080"
        volumes:
            - open-webui:/app/backend/data
        extra_hosts:
            - "host.docker.internal:host-gateway"
        environment:
            - WEBUI_AUTH=False
            - OPENAI_API_KEY=${MASTER_KEY}
            - OPENAI_API_BASE_URL=http://host.docker.internal:4000/v1
        restart: always

    litellm:
        # main-latest tracks recent changes. For critical production, consider finding and using a specific SHA digest or tagged release if available.
        image: ghcr.io/berriai/litellm-database:main-latest
        container_name: litellm
        env_file:
            - .env
        ports:
            - "4000:4000"
        volumes:
            - ./config.yml:/app/config.yaml
        command: --config /app/config.yaml --port 4000
        restart: always
        environment:
            DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
            STORE_MODEL_IN_DB: "True"

    db:
        # Using a specific major version of Postgres (e.g., 16-alpine) is recommended for stability.
        image: postgres:16-alpine
        restart: always
        environment:
            POSTGRES_DB: litellm
            POSTGRES_USER: llmproxy # Consider changing for production and managing via .env
            POSTGRES_PASSWORD: dbpassword9090 # Consider changing for production and managing via .env
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
            interval: 1s
            timeout: 5s
            retries: 10

volumes:
    open-webui:
